{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#  utilities \n",
    "\n",
    "def random_binomial(shape, p=0.0, dtype=None, seed=None):\n",
    "    \"\"\"Returns a tensor with random binomial distribution of values.\n",
    "    # Arguments\n",
    "        shape: A tuple of integers, the shape of tensor to create.\n",
    "        p: A float, `0. <= p <= 1`, probability of binomial distribution.\n",
    "        dtype: String, dtype of returned tensor.\n",
    "        seed: Integer, random seed.\n",
    "    # Returns\n",
    "        A tensor.\n",
    "    \"\"\"\n",
    "    if dtype is None:\n",
    "        dtype = 'float32'\n",
    "    if seed is None:\n",
    "        seed = np.random.randint(10e6)\n",
    "    return tf.where(tf.random_uniform(shape, dtype=dtype, seed=seed) <= p,\n",
    "                    tf.ones(shape, dtype=dtype),\n",
    "                    tf.zeros(shape, dtype=dtype))\n",
    "\n",
    "#reshape Data\n",
    "# sample n frame from videos\n",
    "def my_reshape(data_in ,number_of_frame = 2):\n",
    "    N, M, H, W = data_in.shape\n",
    "    n = M-number_of_frame+1\n",
    "    data_out = np.zeros((n * N, number_of_frame, 64, 64),'uint8')\n",
    "    for j in range(N):\n",
    "        for i in range(n):\n",
    "            data_out[j*n+i] = data_in[j, i:i+number_of_frame]\n",
    "    \n",
    "    return data_out\n",
    "\n",
    "#show filter \n",
    "def dispims(M, height, width, border=0, bordercolor=0.0, layout=None, **kwargs):\n",
    "    from pylab import cm, ceil\n",
    "    numimages = M.shape[1]\n",
    "    if layout is None:\n",
    "        n0 = int(np.ceil(np.sqrt(numimages)))\n",
    "        n1 = int(np.ceil(np.sqrt(numimages)))\n",
    "    else:\n",
    "        n0, n1 = layout\n",
    "    im = bordercolor * np.ones(((height+border)*n0+border,(width+border)*n1+border),dtype='<f8')\n",
    "    for i in range(n0):\n",
    "        for j in range(n1):\n",
    "            if i*n1+j < M.shape[1]:\n",
    "                im[i*(height+border)+border:(i+1)*(height+border)+border,\n",
    "                   j*(width+border)+border :(j+1)*(width+border)+border] = np.vstack((\n",
    "                            np.hstack((np.reshape(M[:,i*n1+j],(height, width)),\n",
    "                                   bordercolor*np.ones((height,border),dtype=float))),\n",
    "                            bordercolor*np.ones((border,width+border),dtype=float)\n",
    "                            ))\n",
    "    pylab.imshow(im, cmap=cm.gray, interpolation='nearest', **kwargs)\n",
    "    pylab.show()\n",
    "    \n",
    "# Manage Data\n",
    "class Dataset:\n",
    "    def __init__(self,data):\n",
    "        self._index_in_epoch = 0\n",
    "        self._epochs_completed = 0\n",
    "        self._data = data\n",
    "        self._num_examples = data.shape[0]\n",
    "        pass\n",
    "\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "\n",
    "    def next_batch(self,batch_size,shuffle = True):\n",
    "        start = self._index_in_epoch\n",
    "        if start == 0 and self._epochs_completed == 0:\n",
    "            idx = np.arange(0, self._num_examples)  # get all possible indexes\n",
    "            np.random.shuffle(idx)  # shuffle indexe\n",
    "            self._data = self.data[idx]  # get list of `num` random samples\n",
    "\n",
    "        # go to the next batch\n",
    "        if start + batch_size > self._num_examples:\n",
    "            self._epochs_completed += 1\n",
    "            rest_num_examples = self._num_examples - start\n",
    "            data_rest_part = self.data[start:self._num_examples]\n",
    "            idx0 = np.arange(0, self._num_examples)  # get all possible indexes\n",
    "            np.random.shuffle(idx0)  # shuffle indexes\n",
    "            self._data = self.data[idx0]  # get list of `num` random samples\n",
    "\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size - rest_num_examples #avoid the case where the #sample != integar times of batch_size\n",
    "            end =  self._index_in_epoch  \n",
    "            data_new_part =  self._data[start:end]  \n",
    "            return np.concatenate((data_rest_part, data_new_part), axis=0)\n",
    "        else:\n",
    "            self._index_in_epoch += batch_size\n",
    "            end = self._index_in_epoch\n",
    "            return self._data[start:end]\n",
    "        \n",
    "numpy_rng = np.random.RandomState(1)\n",
    "SMALL = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LOAD DATA\n",
    "file_name = 'mnist_test_seq.npy'\n",
    "if not os.path.isfile(file_name):\n",
    "    print(\"could not find moving mnist: download it..\")\n",
    "    url = 'http://www.cs.toronto.edu/~nitish/unsupervised_video/mnist_test_seq.npy'\n",
    "    urllib.request.urlretrieve(url, file_name)\n",
    "    print(\"download complete\")\n",
    "else :\n",
    "    print (\"Data Exist\")\n",
    "    \n",
    "data = np.load(file_name)\n",
    "print (data.shape)\n",
    "\n",
    "num_data = data.shape[1]\n",
    "\n",
    "idx =  np.arange(0, num_data)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "num_train = int( 0.7 * num_data)\n",
    "print (num_train)\n",
    "train_idx = idx[0:num_train]\n",
    "test_idx = idx [num_train:num_data]\n",
    "\n",
    "data_train = data[:,train_idx,:,:]\n",
    "data_test = data[:,test_idx,:,:]\n",
    "\n",
    "\n",
    "print (\"data_train \" ,data_train.shape ,\"data_test \" ,data_test.shape   ) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('mean_all.npy'):\n",
    "    # Get mean and variance of all frames \n",
    "    print ('calculate total mean and var')\n",
    "    data_1 = (my_reshape(data, 1)).astype('float32')\n",
    "    v,n,h,w = data_1.shape\n",
    "    data_1 = data_1.reshape((v,h * w))\n",
    "\n",
    "    mean_all = data_1.mean(0)[None,:]\n",
    "    data_1 -= mean_all\n",
    "\n",
    "    var_all = data_1.std(0)  + data_1.std() * 0.1\n",
    "\n",
    "    np.save(\"mean_all\", mean_all)\n",
    "    np.save(\"var_all\", var_all)\n",
    "else :\n",
    "    print ('load mean and var')\n",
    "    mean_all = np.load('mean_all.npy')\n",
    "    var_all = np.load('var_all.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data for first layer pretraining \n",
    "data_1 = my_reshape(data_train, 2)\n",
    "\n",
    "v,n,h,w = data_1.shape\n",
    "data_1 = data_1.reshape((v,n,h * w))\n",
    "print (\"data_1 \" ,data_1.shape)\n",
    "\n",
    "x_dim = data_1.shape[2] \n",
    "\n",
    "ntrain = data_1.shape[0]\n",
    "dataset = Dataset(data_1)\n",
    "data_1 =0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for first layer pretrainig\n",
    "numfac1  = 512\n",
    "nummap1  = 256\n",
    "\n",
    "input_x1 = tf.placeholder(tf.float32, [None, x_dim])\n",
    "input_x2 = tf.placeholder(tf.float32, [None, x_dim])\n",
    "\n",
    "if not os.path.isfile('U1.npy'):\n",
    "    print (\"initialize weights randomly\")\n",
    "    U1 = tf.Variable(tf.random_normal(shape=(x_dim, numfac1)) * 0.01)\n",
    "    V1 = tf.Variable(tf.random_normal(shape=(x_dim, numfac1)) * 0.01)\n",
    "    W1 = tf.Variable(numpy_rng.uniform(low=-0.01, high=+0.01, size=( numfac1, nummap1)).astype('float32'))\n",
    "\n",
    "    bias_W1 = tf.Variable(np.zeros(nummap1, dtype='float32'))\n",
    "else :\n",
    "    print (\"Load weights from file\")\n",
    "    U1 = tf.Variable(np.load(\"U1.npy\"))\n",
    "    V1 = tf.Variable(np.load(\"V1.npy\"))\n",
    "    W1 = tf.Variable(np.load(\"W1.npy\"))\n",
    "\n",
    "    bias_W1 = tf.Variable(np.load(\"bias_W1.npy\"))\n",
    "    \n",
    "\n",
    "# m=sig(W(U*X1 . V*X2 ))\n",
    "M1 =  tf.sigmoid(tf.matmul(tf.multiply(tf.matmul(input_x1,U1) ,tf.matmul(input_x2,V1) ), W1)+ bias_W1)\n",
    "\n",
    "output_x1 = tf.matmul(tf.multiply(tf.matmul(M1,tf.transpose(W1)) ,tf.matmul(input_x2,V1) ),tf.transpose(U1))\n",
    "output_x2 = tf.matmul(tf.multiply(tf.matmul(M1,tf.transpose(W1)) ,tf.matmul(input_x1,U1) ), tf.transpose(V1))\n",
    "\n",
    "#cost_1 = tf.nn.l2_loss(output_x1-input_x1) + tf.nn.l2_loss(output_x2-input_x2)\n",
    "cost_1 = tf.nn.l2_loss(output_x2-input_x2)\n",
    "optimizer_1 = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost_1)\n",
    "\n",
    "U1_normalized = tf.nn.l2_normalize(U1, [0,1], epsilon=1e-12, name=None)\n",
    "V1_normalized = tf.nn.l2_normalize(V1, [0,1], epsilon=1e-12, name=None)\n",
    "\n",
    "normalize_U1 = U1.assign(U1_normalized)\n",
    "normalize_V1 = V1.assign(V1_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "num_batches = int(ntrain/batch_size)\n",
    "training_epochs = 300 \n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    import pylab\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for epoch in range(training_epochs):\n",
    "        total_cost = 0\n",
    "        \n",
    "        fig = pylab.figure(figsize=(20, 20)) \n",
    "        pylab.subplot(1, 2, 1)\n",
    "        dispims(U1.eval(sess), 64, 64, 2)\n",
    "        fig = pylab.figure(figsize=(20, 20)) \n",
    "        pylab.subplot(1, 2, 2)\n",
    "        dispims(V1.eval(sess), 64, 64, 2)\n",
    "        \n",
    "        for batch_id in range(num_batches):\n",
    "            batch = (dataset.next_batch(batch_size)).astype('float32')\n",
    "            batch_x1s = batch[:,0,:]\n",
    "            batch_x2s = batch[:,1,:]\n",
    "            \n",
    "            batch_x1s -= mean_all\n",
    "            batch_x2s -= mean_all\n",
    "            \n",
    "            batch_x1s /= var_all\n",
    "            batch_x2s /= var_all\n",
    "     \n",
    "            sess.run(optimizer_1, feed_dict={input_x1: batch_x1s, input_x2: batch_x2s})\n",
    "            sess.run(normalize_U1)\n",
    "            sess.run(normalize_V1)\n",
    "\n",
    "            cost_ = sess.run(cost_1, feed_dict={input_x1: batch_x1s.astype('float32'), input_x2: batch_x2s.astype('float32')}) \n",
    "            total_cost += cost_\n",
    "            print (\"I: %03d/%03d  E: %03d i:%03d/%03d cost: %.9f\" % (epoch*num_batches+batch_id,training_epochs*num_batches ,epoch,batch_id,num_batches,cost_/batch_size) ) \n",
    "\n",
    "        fig = pylab.figure(figsize=(20, 20)) \n",
    "        pylab.subplot(1, 2, 1)\n",
    "        dispims(U1.eval(sess), 64, 64, 2)\n",
    "        fig = pylab.figure(figsize=(20, 20)) \n",
    "        pylab.subplot(1, 2, 2)\n",
    "        dispims(V1.eval(sess), 64, 64, 2)\n",
    "        print (\"Epoch: %03d/%03d cost: %.9f\" % (epoch,training_epochs ,total_cost / ntrain) ) \n",
    "        np.save(\"U1\", np.array(U1.eval(sess)))\n",
    "        np.save(\"V1\", np.array(V1.eval(sess)))\n",
    "        np.save(\"W1\", np.array(W1.eval(sess)))\n",
    "        np.save(\"bias_W1\", np.array(bias_W1.eval(sess)))\n",
    "\n",
    "dataset=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model for first order relational Auto Encoder \n",
    "input_x1 = tf.placeholder(tf.float32, [None, x_dim])\n",
    "input_x2 = tf.placeholder(tf.float32, [None, x_dim])\n",
    "input_x3 = tf.placeholder(tf.float32, [None, x_dim])\n",
    "\n",
    "if not os.path.isfile('UR1.npy'):\n",
    "    U1 = tf.Variable(np.load(\"U1.npy\"))\n",
    "    V1 = tf.Variable(np.load(\"V1.npy\"))\n",
    "    W1 = tf.Variable(np.load(\"W1.npy\"))\n",
    "    bias_W1 = tf.Variable(np.load(\"bias_W1.npy\"))\n",
    "else:#Continue learning \n",
    "    U1 = tf.Variable(np.load(\"UR1.npy\"))\n",
    "    V1 = tf.Variable(np.load(\"VR1.npy\"))\n",
    "    W1 = tf.Variable(np.load(\"WR1.npy\"))\n",
    "    bias_W1 = tf.Variable(np.load(\"bias_WR1.npy\"))\n",
    "    \n",
    "\n",
    "# m=sig(W(U*X1 . V*X2 ))\n",
    "M1 =  tf.sigmoid(tf.matmul(tf.multiply(tf.matmul(input_x1,U1) ,tf.matmul(input_x2,V1) ), W1)+ bias_W1)\n",
    "\n",
    "output_x3 = tf.matmul(tf.multiply(tf.matmul(M1,tf.transpose(W1)) ,tf.matmul(input_x2,V1) ),tf.transpose(U1))\n",
    "cost_R1 = tf.nn.l2_loss(output_x3-input_x3) \n",
    "\n",
    "optimizer_R1 = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost_R1)\n",
    "\n",
    "U1_normalized = tf.nn.l2_normalize(U1, [0,1], epsilon=1e-12, name=None)\n",
    "V1_normalized = tf.nn.l2_normalize(V1, [0,1], epsilon=1e-12, name=None)\n",
    "\n",
    "normalize_U1 = U1.assign(U1_normalized)\n",
    "normalize_V1 = V1.assign(V1_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data for Second layer pretraining \n",
    "data_2 = my_reshape(data_train, 3) # get 3 frame in each row\n",
    " \n",
    "v,n,h,w = data_2.shape\n",
    "data_2 = data_2.reshape((v,n,h * w))\n",
    "print (\"data_2 \" ,data_2.shape)\n",
    "\n",
    "ntrain = data_2.shape[0]\n",
    "dataset2 = Dataset(data_2)\n",
    "data_2=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "num_batches = int(ntrain/batch_size)\n",
    "training_epochs = 300 \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    import pylab\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for epoch in range(training_epochs):\n",
    "        total_cost = 0\n",
    "        for batch_id in range(num_batches):\n",
    "            batch = (dataset2.next_batch(batch_size)).astype('float32')\n",
    "            batch_x1s = batch[:,0,:]\n",
    "            batch_x2s = batch[:,1,:]\n",
    "            batch_x3s = batch[:,2,:]\n",
    "            \n",
    "            batch_x1s -= mean_all\n",
    "            batch_x2s -= mean_all\n",
    "            batch_x3s -= mean_all\n",
    "            \n",
    "            batch_x1s /= var_all\n",
    "            batch_x2s /= var_all\n",
    "            batch_x3s /= var_all\n",
    "     \n",
    "            sess.run(optimizer_R1, feed_dict={input_x1: batch_x1s, input_x2: batch_x2s, input_x3: batch_x3s})\n",
    "            sess.run(normalize_U1)\n",
    "            sess.run(normalize_V1)\n",
    "\n",
    "            cost_ = sess.run(cost_R1, feed_dict={input_x1: batch_x1s, input_x2: batch_x2s, input_x3: batch_x3s}) \n",
    "            total_cost += cost_\n",
    "            print (\"I: %03d/%03d E: %03d i:%03d/%03d cost: %.9f\" % (epoch*num_batches+batch_id,training_epochs*num_batches ,epoch,batch_id,num_batches,cost_/batch_size) ) \n",
    "\n",
    "        fig = pylab.figure(figsize=(20, 20)) \n",
    "        pylab.subplot(1, 2, 1)\n",
    "        dispims(U1.eval(sess), 64, 64, 2)\n",
    "        fig = pylab.figure(figsize=(20, 20)) \n",
    "        pylab.subplot(1, 2, 2)\n",
    "        dispims(V1.eval(sess), 64, 64, 2)\n",
    "        print (\"Epoch: %03d/%03d cost: %.9f\" % (epoch,training_epochs ,total_cost / ntrain) ) \n",
    "        np.save(\"UR1\", np.array(U1.eval(sess)))\n",
    "        np.save(\"VR1\", np.array(V1.eval(sess)))\n",
    "        np.save(\"WR1\", np.array(W1.eval(sess)))\n",
    "        np.save(\"bias_WR1\", np.array(bias_W1.eval(sess)))\n",
    "        \n",
    "dataset2=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for Second layer pretrainig\n",
    "numfac2  = 512\n",
    "nummap2  = 256\n",
    "\n",
    "#first layer fixed \n",
    "input_x1 = tf.placeholder(tf.float32, [None, x_dim])\n",
    "input_x2 = tf.placeholder(tf.float32, [None, x_dim])\n",
    "input_x3 = tf.placeholder(tf.float32, [None, x_dim])\n",
    "\n",
    "# weights\n",
    "U1 = tf.constant(np.load(\"UR1.npy\"))\n",
    "V1 = tf.constant(np.load(\"VR1.npy\"))\n",
    "W1 = tf.constant(np.load(\"WR1.npy\"))\n",
    "\n",
    "bias_W1 = tf.constant(np.load(\"bias_W1.npy\"))\n",
    "\n",
    "\n",
    "M1_1 =  tf.sigmoid(tf.matmul(tf.multiply(tf.matmul(input_x1,U1) ,tf.matmul(input_x2,V1) ), W1)+ bias_W1)\n",
    "M1_2 =  tf.sigmoid(tf.matmul(tf.multiply(tf.matmul(input_x2,U1) ,tf.matmul(input_x3,V1) ), W1)+ bias_W1)\n",
    "\n",
    "#second layer \n",
    "if not os.path.isfile('U2.npy'): \n",
    "    U2 = tf.Variable(tf.random_normal(shape=(nummap1, numfac2)) * 0.01)\n",
    "    V2 = tf.Variable(tf.random_normal(shape=(nummap1, numfac2)) * 0.01)\n",
    "    W2 = tf.Variable(numpy_rng.uniform(low=-0.01, high=+0.01, size=( numfac2, nummap2)).astype('float32'))\n",
    "\n",
    "    bias_W2 = tf.Variable(np.zeros(nummap2, dtype='float32'))\n",
    "else :#load saved weights and Continue \n",
    "    print (\"Load weights for second layer from file\")\n",
    "    U2 = tf.Variable(np.load(\"U2.npy\"))\n",
    "    V2 = tf.Variable(np.load(\"V2.npy\"))\n",
    "    W2 = tf.Variable(np.load(\"W2.npy\"))\n",
    "\n",
    "    bias_W2 = tf.Variable(np.load(\"bias_W2.npy\"))\n",
    "    \n",
    "    \n",
    "M2 =  tf.sigmoid(tf.matmul(tf.multiply(tf.matmul(M1_1,U2) ,tf.matmul(M1_2,V2) ), W2)+ bias_W2)\n",
    "\n",
    "\n",
    "output_M1_1 = tf.matmul(tf.multiply(tf.matmul(M2,tf.transpose(W2)) ,tf.matmul(M1_2,V2) ),tf.transpose(U2))\n",
    "output_M1_2 = tf.matmul(tf.multiply(tf.matmul(M2,tf.transpose(W2)) ,tf.matmul(M1_1,U2) ),tf.transpose(V2))\n",
    "\n",
    "#cost_2 = tf.nn.l2_loss(output_M1_1-M1_1) + tf.nn.l2_loss(output_M1_2-M1_2)\n",
    "cost_2 =  tf.nn.l2_loss(output_M1_2-M1_2)\n",
    "\n",
    "optimizer_2 = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost_2)\n",
    "\n",
    "U2_normalized = tf.nn.l2_normalize(U2, [0,1], epsilon=1e-12, name=None)\n",
    "V2_normalized = tf.nn.l2_normalize(V2, [0,1], epsilon=1e-12, name=None)\n",
    "\n",
    "normalize_U2 = U2.assign(U2_normalized)\n",
    "normalize_V2 = V2.assign(V2_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data for Second layer pretraining \n",
    "data_2 = my_reshape(data_train, 3) # get 3 frame in each row\n",
    " \n",
    "v,n,h,w = data_2.shape\n",
    "data_2 = data_2.reshape((v,n,h * w))\n",
    "print (\"data_2 \" ,data_2.shape)\n",
    "\n",
    "ntrain = data_2.shape[0]\n",
    "dataset2 = Dataset(data_2)\n",
    "data_2=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "num_batches = int(ntrain/batch_size)\n",
    "training_epochs = 300 \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    import pylab\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for epoch in range(training_epochs):\n",
    "        total_cost = 0\n",
    "        for batch_id in range(num_batches):\n",
    "            batch = (dataset2.next_batch(batch_size)).astype('float32')\n",
    "            batch_x1s = batch[:,0,:]\n",
    "            batch_x2s = batch[:,1,:]\n",
    "            batch_x3s = batch[:,2,:]\n",
    "            \n",
    "            batch_x1s -= mean_all\n",
    "            batch_x2s -= mean_all\n",
    "            batch_x3s -= mean_all\n",
    "            \n",
    "            batch_x1s /= var_all\n",
    "            batch_x2s /= var_all\n",
    "            batch_x3s /= var_all\n",
    "     \n",
    "            sess.run(optimizer_2, feed_dict={input_x1: batch_x1s, input_x2: batch_x2s, input_x3: batch_x3s})\n",
    "            sess.run(normalize_U2)\n",
    "            sess.run(normalize_V2)\n",
    "\n",
    "            cost_ = sess.run(cost_2, feed_dict={input_x1: batch_x1s, input_x2: batch_x2s, input_x3: batch_x3s}) \n",
    "            total_cost += cost_\n",
    "            print (\"I: %03d/%03d E: %03d i:%03d/%03d cost: %.9f\" % (epoch*num_batches+batch_id,training_epochs*num_batches ,epoch,batch_id,num_batches,cost_/batch_size) ) \n",
    "\n",
    "        pylab.figure(figsize=(20, 20)) \n",
    "        pylab.subplot(1, 2, 1)\n",
    "        dispims(U2.eval(sess), 16, 16, 2)\n",
    "        pylab.figure(figsize=(20, 20)) \n",
    "        pylab.subplot(1, 2, 2)\n",
    "        dispims(V2.eval(sess), 16, 16, 2)\n",
    "        print (\"Epoch: %03d/%03d cost: %.9f\" % (epoch,training_epochs ,total_cost / ntrain) ) \n",
    "        np.save(\"U2\", np.array(U2.eval(sess)))\n",
    "        np.save(\"V2\", np.array(V2.eval(sess)))\n",
    "        np.save(\"W2\", np.array(W2.eval(sess)))\n",
    "        np.save(\"bias_W2\", np.array(bias_W2.eval(sess)))\n",
    "        \n",
    "dataset2=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model of  second order relational auto encoder\n",
    "input_x1 = tf.placeholder(tf.float32, [None, x_dim])\n",
    "input_x2 = tf.placeholder(tf.float32, [None, x_dim])\n",
    "input_x3 = tf.placeholder(tf.float32, [None, x_dim])\n",
    "input_x4 = tf.placeholder(tf.float32, [None, x_dim])\n",
    "\n",
    "# weights for first layer URR1\n",
    "if not os.path.isfile('URR1.npy'): \n",
    "    U1 = tf.Variable(np.load(\"UR1.npy\"))\n",
    "    V1 = tf.Variable(np.load(\"VR1.npy\"))\n",
    "    W1 = tf.Variable(np.load(\"WR1.npy\"))\n",
    "    bias_W1 = tf.Variable(np.load(\"bias_WR1.npy\"))\n",
    "\n",
    "    #weights for second layer \n",
    "    U2 = tf.Variable(np.load(\"U2.npy\"))\n",
    "    V2 = tf.Variable(np.load(\"V2.npy\"))\n",
    "    W2 = tf.Variable(np.load(\"W2.npy\"))\n",
    "    bias_W2 = tf.Variable(np.load(\"bias_W2.npy\"))\n",
    "else :\n",
    "    U1 = tf.Variable(np.load(\"URR1.npy\"))\n",
    "    V1 = tf.Variable(np.load(\"VRR1.npy\"))\n",
    "    W1 = tf.Variable(np.load(\"WRR1.npy\"))\n",
    "    bias_W1 = tf.Variable(np.load(\"bias_WRR1.npy\"))\n",
    "\n",
    "    #weights for second layer \n",
    "    U2 = tf.Variable(np.load(\"URR2.npy\"))\n",
    "    V2 = tf.Variable(np.load(\"VRR2.npy\"))\n",
    "    W2 = tf.Variable(np.load(\"WRR2.npy\"))\n",
    "    bias_W2 = tf.Variable(np.load(\"bias_WRR2.npy\"))\n",
    "\n",
    "\n",
    "M1_1 =  tf.sigmoid(tf.matmul(tf.multiply(tf.matmul(input_x1,U1) ,tf.matmul(input_x2,V1) ), W1)+ bias_W1)\n",
    "M1_2 =  tf.sigmoid(tf.matmul(tf.multiply(tf.matmul(input_x2,U1) ,tf.matmul(input_x3,V1) ), W1)+ bias_W1)\n",
    "\n",
    "M2 =  tf.sigmoid(tf.matmul(tf.multiply(tf.matmul(M1_1,U2) ,tf.matmul(M1_2,V2) ), W2)+ bias_W2)\n",
    "\n",
    "output_M1_3 = tf.matmul(tf.multiply(tf.matmul(M2,tf.transpose(W2)) ,tf.matmul(M1_2,V2) ),tf.transpose(U2))\n",
    "output_x4 = tf.matmul(tf.multiply(tf.matmul(output_M1_3,tf.transpose(W1)) ,tf.matmul(input_x3,V1) ),tf.transpose(U1))\n",
    "\n",
    "cost_RR = tf.nn.l2_loss(output_x4-input_x4)\n",
    "\n",
    "optimizer_RR = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost_RR)\n",
    "\n",
    "U1_normalized = tf.nn.l2_normalize(U1, [0,1], epsilon=1e-12, name=None)\n",
    "V1_normalized = tf.nn.l2_normalize(V1, [0,1], epsilon=1e-12, name=None)\n",
    "U2_normalized = tf.nn.l2_normalize(U2, [0,1], epsilon=1e-12, name=None)\n",
    "V2_normalized = tf.nn.l2_normalize(V2, [0,1], epsilon=1e-12, name=None)\n",
    "\n",
    "normalize_U1 = U1.assign(U1_normalized)\n",
    "normalize_V1 = V1.assign(V1_normalized)\n",
    "normalize_U2 = U2.assign(U2_normalized)\n",
    "normalize_V2 = V2.assign(V2_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data for second order relational auto encoder\n",
    "data_2 = my_reshape(data_train, 4) # get 4 frame in each row\n",
    " \n",
    "v,n,h,w = data_2.shape\n",
    "data_2 = data_2.reshape((v,n,h * w))\n",
    "print (\"data_2 \" ,data_2.shape)\n",
    "\n",
    "ntrain = data_2.shape[0]\n",
    "dataset2 = Dataset(data_2)\n",
    "data_2=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "num_batches = int(ntrain/batch_size)\n",
    "training_epochs = 300 \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    import pylab\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for epoch in range(training_epochs):\n",
    "        total_cost = 0\n",
    "        for batch_id in range(num_batches):\n",
    "            batch = (dataset2.next_batch(batch_size)).astype('float32')\n",
    "            batch_x1s = batch[:,0,:]\n",
    "            batch_x2s = batch[:,1,:]\n",
    "            batch_x3s = batch[:,2,:]\n",
    "            batch_x4s = batch[:,3,:]\n",
    "            \n",
    "            batch_x1s -= mean_all\n",
    "            batch_x2s -= mean_all\n",
    "            batch_x3s -= mean_all\n",
    "            batch_x4s -= mean_all\n",
    "            \n",
    "            batch_x1s /= var_all\n",
    "            batch_x2s /= var_all\n",
    "            batch_x3s /= var_all\n",
    "            batch_x4s /= var_all\n",
    "     \n",
    "            sess.run(optimizer_RR, feed_dict={input_x1: batch_x1s, input_x2: batch_x2s, input_x3: batch_x3s, input_x4: batch_x4s})\n",
    "            sess.run(normalize_U1)\n",
    "            sess.run(normalize_V1)\n",
    "            sess.run(normalize_U2)\n",
    "            sess.run(normalize_V2)\n",
    "            cost_ = sess.run(cost_RR, feed_dict={input_x1: batch_x1s, input_x2: batch_x2s, input_x3: batch_x3s, input_x4: batch_x4s}) \n",
    "            total_cost += cost_\n",
    "            print (\"I: %03d/%03d E: %03d i:%03d/%03d cost: %.9f\" % (epoch*num_batches+batch_id,training_epochs*num_batches ,epoch,batch_id,num_batches,cost_/batch_size) ) \n",
    "\n",
    "        pylab.figure(figsize=(20, 20)) \n",
    "        pylab.subplot(1, 2, 1)\n",
    "        dispims(U1.eval(sess), 64, 64, 2)\n",
    "        pylab.figure(figsize=(20, 20)) \n",
    "        pylab.subplot(1, 2, 2)\n",
    "        dispims(V1.eval(sess), 64, 64, 2)\n",
    "        \n",
    "        pylab.figure(figsize=(20, 20)) \n",
    "        pylab.subplot(1, 2, 1)\n",
    "        dispims(U2.eval(sess), 16, 16, 2)\n",
    "        pylab.figure(figsize=(20, 20)) \n",
    "        pylab.subplot(1, 2, 2)\n",
    "        dispims(V2.eval(sess), 16, 16, 2)\n",
    "        print (\"Epoch: %03d/%03d cost: %.9f\" % (epoch,training_epochs ,total_cost / ntrain) )\n",
    "        np.save(\"URR1\", np.array(U1.eval(sess)))\n",
    "        np.save(\"VRR1\", np.array(V1.eval(sess)))\n",
    "        np.save(\"WRR1\", np.array(W1.eval(sess)))\n",
    "        np.save(\"bias_WRR1\", np.array(bias_W1.eval(sess)))\n",
    "        np.save(\"URR2\", np.array(U2.eval(sess)))\n",
    "        np.save(\"VRR2\", np.array(V2.eval(sess)))\n",
    "        np.save(\"WRR2\", np.array(W2.eval(sess)))\n",
    "        np.save(\"bias_WRR2\", np.array(bias_W2.eval(sess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
